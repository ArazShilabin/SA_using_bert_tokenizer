{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_bert_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2hBCyP37DfTJ",
        "ea-nMBIMAV7_",
        "f88TsAbxEAv6",
        "0JtgEviBAcew",
        "FPIWIoBeDlwn",
        "EdNs9Mns7ERL",
        "FIpeakPP7Gci",
        "50fQSLEl72OE",
        "PJV12RZ4EKL_",
        "f_TSVOsv-K55",
        "CvWEiZxw-QGn",
        "o1-yytKIDe75",
        "kTgdxwC7I26L",
        "dUrIOeUsIoQw",
        "1piGn2MrK5qp",
        "x-r2to4fO2Y6",
        "mHVo1_-FO6hr",
        "7AcQbsXERDTd",
        "f0uirgCUTVy6",
        "qM5hWUTnEJ6H",
        "agNUC9xToAcR",
        "PWnNKVDboIpa",
        "CCFdXyabpAkw",
        "olKb-XUHpyri"
      ],
      "authorship_tag": "ABX9TyOmuKN+yqxWFNnrf2lQ4TvE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArazShilabin/SA_using_bert_tokenizer/blob/main/SA_bert_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hBCyP37DfTJ"
      },
      "source": [
        "### mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at00gH74_82C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "56fc70fc-1772-41c2-d5ba-acab13ed2a2e"
      },
      "source": [
        "\"\"\" \r\n",
        "Use this javascript code in inspect>console so you wont need to click the page every 15 min:\r\n",
        "\r\n",
        "########################\r\n",
        "function ConnectButton(){\r\n",
        "    console.log(\"Connect pushed\"); \r\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \r\n",
        "}\r\n",
        "setInterval(ConnectButton,60000);\r\n",
        "########################\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%pwd\r\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea-nMBIMAV7_"
      },
      "source": [
        "### change current path to where the working project folder is at"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax6n6V_MAOYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e8b792c3-ab48-40af-ab35-e15b40bca8bd"
      },
      "source": [
        "%cd drive/MyDrive/projects/bert_tokenizer_SA/\r\n",
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/projects/bert_tokenizer_SA\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/projects/bert_tokenizer_SA'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88TsAbxEAv6"
      },
      "source": [
        "# Step 0: Get The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JtgEviBAcew"
      },
      "source": [
        "### upload the data to our current path and unzip it (uncomment and run this only once)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEQhpE2SAdM_"
      },
      "source": [
        "# # data is from: http://help.sentiment140.com/for-students you can use this or just upload your own data\r\n",
        "# %cd data\r\n",
        "# !wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip # download\r\n",
        "# !unzip trainingandtestdata.zip # unzip the downloaded file\r\n",
        "# %cd .. \r\n",
        "# %pwd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPIWIoBeDlwn"
      },
      "source": [
        "# Step 1: Importing Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdNs9Mns7ERL"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DMMjWcHCtTx"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import math\r\n",
        "import time\r\n",
        "from bs4 import BeautifulSoup # we use this library to turn the tweets to texts\r\n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIpeakPP7Gci"
      },
      "source": [
        "### Install Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwSlc2H67aux",
        "outputId": "72c6c453-1021-4b32-f423-e7d9e1e69d06"
      },
      "source": [
        "# this is not the official one, it's a lighter one\r\n",
        "!pip install bert-for-tf2\r\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.7)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.94)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fQSLEl72OE"
      },
      "source": [
        "### Add tensorflow packages and Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA5szEBu77M6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers\r\n",
        "import tensorflow_hub as hub # all the pretrained models are installed in tensorflow hub\r\n",
        "import bert"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJV12RZ4EKL_"
      },
      "source": [
        "# Step 2: Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_TSVOsv-K55"
      },
      "source": [
        "## A) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXDoO-XVEvf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "59af87f5-01e0-47e4-cf7e-528c6b2d6ff5"
      },
      "source": [
        "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\r\n",
        "data = pd.read_csv(\r\n",
        "    \"data/training.1600000.processed.noemoticon.csv\",\r\n",
        "    header=None,\r\n",
        "    names=cols,\r\n",
        "    engine=\"python\", # we need this for this specific data\r\n",
        "    encoding=\"latin1\") # the encoded format\r\n",
        "\r\n",
        "data.drop([\"id\",\"date\",\"query\",\"user\"], axis=1, inplace=True) # drop the useless cols\r\n",
        "data.head(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentiment                                               text\n",
              "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
              "1          0  is upset that he can't update his Facebook by ...\n",
              "2          0  @Kenichan I dived many times for the ball. Man...\n",
              "3          0    my whole body feels itchy and like its on fire \n",
              "4          0  @nationwideclass no, it's not behaving at all...."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvWEiZxw-QGn"
      },
      "source": [
        "## B) Clean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQw8ePJ-U6C"
      },
      "source": [
        "def clean_tweet(tweet):\r\n",
        "    # the lmxl is the encoding it has, so beautifulsoup turns it to readable txt\r\n",
        "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\r\n",
        "    # remove the @'s (*: appear 0 or more)\r\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9]*\", ' ', tweet)\r\n",
        "    # remove \"http://\" or \"https://\" (this s? means that 's' can be or not)\r\n",
        "    tweet = re.sub(r\"https?://[A-Za-z0-9]*\", ' ', tweet)\r\n",
        "    # get rid of everything that is not a letter or \".!?'\"\r\n",
        "    tweet = re.sub(r\"[^a-zA-Z.!?']\",' ', tweet)\r\n",
        "    # replace >=2 white spaces with one (+: appear at least once)\r\n",
        "    tweet = re.sub(\" +\", ' ', tweet)\r\n",
        "    return tweet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syKavfUiArJu"
      },
      "source": [
        "data_clean = [clean_tweet(tweet) for tweet in data['text']]\r\n",
        "data_labels = data['sentiment'].values\r\n",
        "data_labels[data_labels == 4] = 1 # for some reason the labels instead of 0's and 1's, they are 0's and 4's so we turn the 4's to 1's"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1-yytKIDe75"
      },
      "source": [
        "## C) Tokenization (using Bert)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTgdxwC7I26L"
      },
      "source": [
        "### Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbkHGQBSA8Ho"
      },
      "source": [
        "FullTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "# all the pretrained models are installed in tensorflow hub, we can find many \r\n",
        "# diffenet kinds in Hub, we can choose differet versions from hub, now we \r\n",
        "# got the weights of the layer (these are the SavedModels)\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "                            trainable=False)\r\n",
        "\r\n",
        "# get the vocabulary from the layer\r\n",
        "vocab_file =  bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # do lowercase\r\n",
        "tokenizer = FullTokenizer(vocab_file, do_lower_case) # get the tokenizer\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-9Jt1heJ6xi"
      },
      "source": [
        "def encode_sentences(sentence): # e.g. 'tensorflow is' -> [23435, 12314, 2003]\r\n",
        "    # the full explenation is in the 'Test our tokenizer' section\r\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL4ZZwCFKL7R"
      },
      "source": [
        "data_inputs = [encode_sentences(sentence) for sentence in data_clean] # encode inputs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUrIOeUsIoQw"
      },
      "source": [
        "### Test our tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCXY_Ny0InIJ",
        "outputId": "f4a7c94d-74d4-417b-c7cb-dd977170e4d0"
      },
      "source": [
        "test_text = \"Tensorflow is great, ain't it?\"\r\n",
        "print(tokenizer.tokenize(test_text))\r\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_text)))\r\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['tensor', '##flow', 'is', 'great', ',', 'ain', \"'\", 't', 'it', '?']\n",
            "[23435, 12314, 2003, 2307, 1010, 7110, 1005, 1056, 2009, 1029]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1piGn2MrK5qp"
      },
      "source": [
        "## D) Dataset creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-r2to4fO2Y6"
      },
      "source": [
        "### Shuffle and get rid of small datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBwBR980LOOB"
      },
      "source": [
        "# we get [text_encoded, label, len_text] we want this for padding purposes later\r\n",
        "data_with_len = [[sent, data_labels[i], len(sent)]\r\n",
        "                 for i, sent in enumerate(data_inputs)]\r\n",
        "random.shuffle(data_with_len) # we shuffle the data because it's labels were 000....0011....111\r\n",
        "data_with_len.sort(key=lambda x: x[2]) # we sort it based on lenghts\r\n",
        "sorted_all = [(sent_lab[0], sent_lab[1])\r\n",
        "              for sent_lab in data_with_len if sent_lab[2] > 3] # short sentences probably have no sentiment"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHVo1_-FO6hr"
      },
      "source": [
        "### Dataset generator (because our data isn't the same size, needs padding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU-av4nCL3d6",
        "outputId": "62d80df9-0655-48a8-bb50-e4f928610b18"
      },
      "source": [
        "all_dataset = tf.data.Dataset.from_generator(generator=lambda: sorted_all, # generator needs to be callable\r\n",
        "                                             output_types=(tf.int32,tf.int32)) # (inputs, labels) both are int\r\n",
        "print(type(all_dataset))\r\n",
        "print(next(iter(all_dataset)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.FlatMapDataset'>\n",
            "(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 2026,  2132, 13403,  1012], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AcQbsXERDTd"
      },
      "source": [
        "### Batching & Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zO4YaB-RFu2",
        "outputId": "c1fbd083-09c3-45ee-c72b-568c51269e48"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "# turn it to padded\r\n",
        "all_batched = all_dataset.padded_batch(\r\n",
        "                batch_size=BATCH_SIZE,\r\n",
        "                padded_shapes=((None,),())\r\n",
        "                # None here means pad the first dim and keep the same vals for the next dims.\r\n",
        "                # for output we dont want to use it for padding so we choose it to be empty\r\n",
        ")\r\n",
        "print(type(all_batched))\r\n",
        "print(next(iter(all_batched)))\r\n",
        "# here they are all size 4 because it's the first batch, but later there\r\n",
        "# will be a mixture of size for example 3&4 so the 3's will be padded to 4"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset'>\n",
            "(<tf.Tensor: shape=(32, 4), dtype=int32, numpy=\n",
            "array([[ 2026,  2132, 13403,  1012],\n",
            "       [ 6160,  3582, 19338,  2063],\n",
            "       [ 2039,  2001,  7078,  6429],\n",
            "       [ 2017, 20482,  8808,  1029],\n",
            "       [ 2204,  2851, 10474,  2155],\n",
            "       [ 2074,  2288,  8412,  7136],\n",
            "       [ 5404,  4989, 19237,  2295],\n",
            "       [ 3335,  2078, 22794, 22017],\n",
            "       [ 2003,  5962,  2000,  2189],\n",
            "       [ 1045,  2540,  2703,  5232],\n",
            "       [ 2746,  2091,  2007,  2242],\n",
            "       [ 1045,  2342,  8771,  2205],\n",
            "       [ 2012,  2026,  4624,  2551],\n",
            "       [ 2003,  1999,  3009,  3499],\n",
            "       [ 1045,  2439,  2026,  4950],\n",
            "       [ 1060,  2033,  2205,  2080],\n",
            "       [ 2307, 19431,  4037,   999],\n",
            "       [ 1043,  2305,   999,  1060],\n",
            "       [ 2107,  1037,  4030,  2154],\n",
            "       [ 2053,  2028,  2182,  1029],\n",
            "       [ 2204,  2851,  3071,   999],\n",
            "       [ 5477,  2197,  4957,  3631],\n",
            "       [ 1045,  2342,  1037,  2166],\n",
            "       [ 1045,  2572,  2085,   999],\n",
            "       [10722, 20360,  4937,  2351],\n",
            "       [ 2017,  2024,  2525,  2589],\n",
            "       [ 2013,  2073,   999,  1029],\n",
            "       [ 1998,  4283,  1037,  2843],\n",
            "       [ 2293,  1996,  2047,  6140],\n",
            "       [ 2025,  2074,  2017,  1012],\n",
            "       [14556, 10882,  2140,  1037],\n",
            "       [ 3403,  2005,  1037,  2795]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
            "array([0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0], dtype=int32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0uirgCUTVy6"
      },
      "source": [
        "### train-test split for batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azpgwT5BTWHg"
      },
      "source": [
        "NB_BATCHES = math.ceil(len(sorted_all)/BATCH_SIZE) # get number of batches (ceil(3.2)==4)\r\n",
        "NB_BATCHES_TEST = NB_BATCHES // 10 # we just want the 10% of the data for testing purposes\r\n",
        "all_batched.shuffle(NB_BATCHES) # shuffle it (we give it NB_BATCHES, and not NB_BATCHES*batch_size because we want to keep the integrity of each batch)\r\n",
        "test_data_set = all_batched.take(NB_BATCHES_TEST) # take the fist 10% as test\r\n",
        "train_data_set = all_batched.skip(NB_BATCHES_TEST) # skip the 10% to get the other 90% as train"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM5hWUTnEJ6H"
      },
      "source": [
        "# Step 3: Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywbYhlVQExPY"
      },
      "source": [
        "class DCNN(tf.keras.Model):\r\n",
        "    def __init__(self,\r\n",
        "                 vocab_size,\r\n",
        "                 emb_dim=128,\r\n",
        "                 nb_filters=50,\r\n",
        "                 FFN_units=512,\r\n",
        "                 nb_classes=2,\r\n",
        "                 dropout_rate=0.1,\r\n",
        "                 training=False,\r\n",
        "                 name='dcnn'):\r\n",
        "        super(DCNN, self).__init__(name=name)\r\n",
        "        \r\n",
        "        self.embedding = layers.Embedding(vocab_size, emb_dim)\r\n",
        "        \r\n",
        "        # bigram is conv on 2-grams (kernel_size=2)\r\n",
        "        self.bigram = layers.Conv1D(filters=nb_filters,\r\n",
        "                                    kernel_size=2,\r\n",
        "                                    padding='valid',\r\n",
        "                                    activation='relu')\r\n",
        "\r\n",
        "        self.trigram =  layers.Conv1D(filters=nb_filters,\r\n",
        "                                      kernel_size=3,\r\n",
        "                                      padding='valid',\r\n",
        "                                      activation='relu')\r\n",
        "        \r\n",
        "        self.fourgram =  layers.Conv1D(filters=nb_filters,\r\n",
        "                                       kernel_size=4,\r\n",
        "                                       padding='valid',\r\n",
        "                                       activation='relu')\r\n",
        "        \r\n",
        "        self.pool = layers.GlobalMaxPool1D()     \r\n",
        "\r\n",
        "        self.dense_1 = layers.Dense(units=FFN_units,\r\n",
        "                                    activation='relu')\r\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\r\n",
        "\r\n",
        "        # u could use softmax for binary too, but sigmoid with only 1 unit is\r\n",
        "        # used always so why not just use it here too...\r\n",
        "        if nb_classes == 2:\r\n",
        "            self.last_dense = layers.Dense(units=1, activation='sigmoid')\r\n",
        "        else:\r\n",
        "            self.last_dense = layers.Dense(units=nb_classes,\r\n",
        "                                           activation='softmax')\r\n",
        "\r\n",
        "    def call(self, inputs, training):\r\n",
        "        # training is boolean, when it's false we don't do dropout\r\n",
        "        x = self.embedding(inputs) # (batch_size, d_embeded(128))\r\n",
        "\r\n",
        "        # we get outputs for all 3 kernelsizes independentaly (we concat them later)\r\n",
        "        x_1 = self.bigram(x)\r\n",
        "        x_1 = self.pool(x_1) # (batch_size, nb_filters, 128-2+1)\r\n",
        "\r\n",
        "        x_2 = self.trigram(x)\r\n",
        "        x_2 = self.pool(x_2) # (batch_size, nb_filters, 128-3+1)\r\n",
        "\r\n",
        "        x_3 = self.fourgram(x)\r\n",
        "        x_3 = self.pool(x_3) # (batch_size, nb_filters, 128-4+1)\r\n",
        "\r\n",
        "        merged = layers.concatenate([x_1,x_2,x_3], axis=-1) # (batch_size, 3 * nb_filters, (128*3-2-3-4+3))\r\n",
        "\r\n",
        "        merged = self.dense_1(merged)\r\n",
        "        merged = self.dropout(merged, training=training)\r\n",
        "        \r\n",
        "        output = self.last_dense(merged)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xnXGfBdEJxn"
      },
      "source": [
        "# Step 4: Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agNUC9xToAcR"
      },
      "source": [
        "### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL_v0AkhExtY"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.vocab)\r\n",
        "EMB_DIM = 200\r\n",
        "NB_FILTERS = 100\r\n",
        "FFN_UNITS = 256\r\n",
        "NB_CLASSES = 2\r\n",
        "DROPOUT_RATE = 0.2\r\n",
        "NB_EPOCHS = 1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWnNKVDboIpa"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7l6lZZBoKah"
      },
      "source": [
        "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\r\n",
        "            emb_dim=EMB_DIM,\r\n",
        "            nb_filters=NB_FILTERS,\r\n",
        "            FFN_units=FFN_UNITS,\r\n",
        "            nb_classes=NB_CLASSES,\r\n",
        "            dropout_rate=DROPOUT_RATE)\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whb4wPB8ovKx"
      },
      "source": [
        "if NB_CLASSES == 2:\r\n",
        "     Dcnn.compile(loss='binary_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['accuracy'])    \r\n",
        "else:\r\n",
        "     Dcnn.compile(loss='sparse_categorical_crossentropy',\r\n",
        "                  optimizer='adam',\r\n",
        "                  metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCFdXyabpAkw"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daZp6vLUo_mp",
        "outputId": "1ba8fe15-6931-41bc-f07c-a2fc61628a12"
      },
      "source": [
        "checkpoint_path = \"ckpt\"\r\n",
        "\r\n",
        "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\r\n",
        "\r\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\r\n",
        "\r\n",
        "if ckpt_manager.latest_checkpoint:\r\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
        "    print(\"Latest chekpoint has been restored!\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest chekpoint has been restored!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olKb-XUHpyri"
      },
      "source": [
        "### Custom call back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvigWyspsLp"
      },
      "source": [
        "class MyCustomCallback(tf.keras.callbacks.Callback):\r\n",
        "    # we basically tell tensorflow what it should print or do after each epoch ends\r\n",
        "    # this on_epoch_end actually exists in tf.keras.callbacks so we just call it\r\n",
        "    # that MyCustomCallback has inherited\r\n",
        "    def on_epoch_end(self, epoch, logs=None):\r\n",
        "        ckpt_manager.save()\r\n",
        "        print(f\"Checkpoint saved!\")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_BOLP0qgwZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_SeS-eFqjNQ",
        "outputId": "9f4a0d96-8a04-4285-ac06-08feff0a94dc"
      },
      "source": [
        "Dcnn.fit(train_data_set,\r\n",
        "         validation_data=test_data_set,\r\n",
        "         epochs=NB_EPOCHS,\r\n",
        "         callbacks=[MyCustomCallback()])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43418/43418 [==============================] - 4442s 102ms/step - loss: 0.2129 - accuracy: 0.9137 - val_loss: 0.4386 - val_accuracy: 0.8437\n",
            "Checkpoint saved!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9dc3ccf748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jSyyYBxEJP3"
      },
      "source": [
        "# Step 5: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWhPXB_ouJy7"
      },
      "source": [
        "def predict(example_sentence):\r\n",
        "    example_sentence = encode_sentences(example_sentence)\r\n",
        "    # we add this extra dim to simulate the batch\r\n",
        "    example_sentence = np.expand_dims(example_sentence,axis=0)\r\n",
        "    prediction = Dcnn(example_sentence, training=False)\r\n",
        "    # the prediction will be between 0 to 1 ([0 to 0.5] is 0 AND [0.5 to 1] is 1)\r\n",
        "    # so we mult it by 2 then do floor to either get 0 or 1\r\n",
        "    Sentiment = math.floor(prediction*2)\r\n",
        "    if Sentiment >= 1:\r\n",
        "        print(f\"positive - certainty: {(prediction - 0.5) * 2.0}\")\r\n",
        "    else:\r\n",
        "        print(f\"negative - certainty: {(0.5 - prediction) * 2.0}\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37H-xvxWueZ5",
        "outputId": "bdfb887f-19b8-4a36-c9bc-9a89f0502e74"
      },
      "source": [
        "negative_example = \"The world is ending\"\r\n",
        "predict(negative_example)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative - certainty: [[0.8656031]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms9ydUA30fqh",
        "outputId": "6f4adf62-e3b4-4a6b-b00a-78135348bf1f"
      },
      "source": [
        "negative_example = \"this world is a great place to live in\"\r\n",
        "predict(negative_example)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive - certainty: [[0.9342562]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwLmnkUfwihV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
